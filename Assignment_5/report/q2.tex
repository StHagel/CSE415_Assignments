\subsection*{Question 2a:}\label{sec:2a}

Just as in question 1b, it took 8 steps for the initial state to have a non zero value.

\subsection*{Question 2b:}\label{sec:2b}

This question does not exist in the assignment.

\subsection*{Question 2c:}\label{sec:2c}

The policy shown results in finding the quickest path from each state.
Therefore it is already the optimal policy.
This happens, because the state values, and also their Q-values, are increased move by move, so the first policy, in which every state has been updated at least once, will already know the fastest path from each state.

\subsection*{Question 2d:}\label{sec:2d}

After 56 steps, delta\_max was lower than $10^{-8}$, so the MDP was considered as converged.

\subsection*{Question 2e:}\label{sec:2e}

The policy stays the same after the additional VI steps, since the old policy was already optimal, as explained in question 2c.
